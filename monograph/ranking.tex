Como dito anteriormente, a técnica de \emph{ranking reduzido a classificação} aplica uma transformação à base original antes da etapa de treinamento do classificador e modifica a etapa de avaliação a fim de ordenar as instâncias. A transformação da base e o treinamento do classificador compõem um algoritmo de nome \emph{AUC-Train}, traduzido a partir de \cite{langford08} no algoritmo \ref{alg:auc-train}.

\begin{algorithm}[h!]
    Seja $S' = \{\langle (x_1, x_2), 1(y_1 < y_2) \rangle : (x_1, y_1), (x_2, y_2) \in S \; e \;  y_1 \neq y_2\}$\;
    \Retorna{$c = A(S')$}
    
    \caption{AUC-Train}
    \label{alg:auc-train}
\end{algorithm}

No algoritmo \ref{alg:auc-train}, $S$ é a base de treinamento; $S'$ é uma transformação em cima da base de treinamento; $x$ e $y$ são, respectivamente, o vetor de atributos e a classe de uma instância $\langle x, y \rangle$ e $A$ é um algoritmo capaz de aprender um classificador.

A transformação consiste em formar novas instâncias a partir de combinações entre instâncias de classes diferentes. A ordem das instâncias  na combinação é relevante: o par $\langle (x_1, x_2), 1(y_1 < y_2) \rangle$ e o par $\langle (x_2, x_1), 1(y_2 < y_1) \rangle$ representam informações diferentes para o aprendizado do classificador.

Essa transformação foi implantada particionando a base de treinamento em dois subconjuntos, um subconjunto $S_0$ com as instâncias de classe $0$ e outro, $S_1$, com as instâncias de classe $1$. Com as duas partições definidas, basta combiná-las em um conjunto de elementos com formato final $\langle (x_{\alpha}, X_{\beta}), 1(y_{\alpha} < y_{\beta}) \rangle$.

A \emph{função $\ref{func:one}$}, aplicada para obter a classe da nova instância, retorna $1$, se o resultado da expressão avaliada é verdadeiro, ou $0$, se o resultado da expressão é falso. Como o argumento para a função é $y_{\alpha} < y_{\beta}$, se $y_{\alpha}$ valer $1$, a classe da nova instância será $0$, se valer $0$, a classe da nova instância será $1$.

\begin{function}[h!]
    $avaliacao \gets 1$\;

    \Se{$expr = False$}{
        $avaliacao \gets 0$\;
    }

    \Retorna{$avaliacao$}

    \caption{1($expr$)}
    \label{func:one}
\end{function}

A modificação da etapa de avaliação tem como objetivo usar as previsões do classificador treinado pelo \emph{algoritmo \emph{AUC-Train}} para gerar o \emph{ranking} da base de avaliação. O algoritmo que efetua a ordenação recebe o nome de \emph{Tournament (torneio)}. O porquê do nome é devido ao algoritmo se assemelhar a um torneio em que muitos competidores se enfrentam, formando um \emph{ranking} de acordo com suas performances.

Continuando a analogia com uma competição, a performance de cada instância na ordenação é medida como a quantidade de "vitórias" que essa instância obteve diante das outras instâncias na base a ser ordenada. Para gerar o \emph{ranking}, basta promover o embate entre todas as instâncias na base e ordenar pelo número de "vitórias".

Como desejamos ordenar as instâncias de forma que as que tenham maior chance de pertencer à classe $0$ estejam no topo, a semântica de "vitoria" é: em um embate entre duas instâncias, a instância vencedora é a que possui a maior chance de pertencer à classe $0$. Quem define a instância vitoriosa em um embate é a previsão do classificador binário.

Tecnicamente, um embate entre duas instâncias com atributos representados por $x$ e $x'$ consiste em criar uma nova instância com formato $\langle (x, x') \rangle$ e submetê-la a classificação. Se a classe prevista da nova instância for $1$, $x$ vence, se for $0$, $x'$ vence. Nota-se que o formato da instância a ser classificada é o mesmo que o de uma instância usada no treinamento do classificador, exceto pela ausência de classe. O algoritmo \ref{alg:tournament}, traduzido de \cite{langford08} demonstra o processo de ordenação.

\begin{algorithm}[h!]
    Para $x \in U$, seja $grau(x) = |\{x':c(x, x') = 1, x' \in U\}|$\;
    Ordenar U em ordem decrescente de $grau(x)$, resolvendo empates arbitrariamente
    
    \caption{Tournament}
    \label{alg:tournament}
\end{algorithm}

No algortimo \ref{alg:tournament}, $U$ é uma base composta por instâncias sem classes assinaladas; $x$ e $x'$ são vetores de atributos de duas instâncias em um embate; $c$ é o classificador aprendido pela etapa \emph{AUC-Train} e $grau(x)$ é a quantidade de "vitórias" obtidas pela instância $x$ sobre as demais instâncias em $U$.

Uma das vantagens da técnica de \emph{ranking reduzido a classificação} é que o uso de um algoritmo de aprendizado para classificação é transparente. Essa característica possibilitou o uso dos algoritmos de classificação existentes na ferramenta \emph{WEKA}.

O principal ponto negativo é a performance da técnica no que diz respeito a tempo computacional. Os algoritmos \ref{alg:auc-train} \emph{(AUC-Train)} e \ref{alg:tournament} \emph{(Tournament)} que constituem o cerne da técnica de \emph{ranking reduzido a classificação} possuem elevada complexidade.

Os algoritmos expostos neste capítulo estão descritos em pseudo-código e em sua maioria se baseiam em conceitos de teoria de conjuntos e vetores; uma vez que as principais estruturas manipuladas, base e instâncias, foram definidas sobre esses conceitos.

Assim como a maioria das técnicas de aprendizado supervisionada, o \emph{ranking reduzido a classificação} pode ser dividido em duas grandes partes distintas: a primeira é o treinamento de um ordenador e a segunda é a ordenação de uma base a partir do ordenador treinado.

Como os algoritmos nas etapas de treinamento e ordenação para a técnica de \emph{ranking reduzido a classificação} são, respectivamente, \emph{AUC-Train} e \emph{Tournament}; a complexidade da técnica será estudada a partir das complexidades desses dois algoritmos separadamente.


\section{Treinamento: Estudo de complexidade do algoritmo \emph{AUC-Train}}

Considera-se como entrada para o algoritmo \emph{AUC-Train} uma base $S$ com $n$ instâncias das quais $k$ possuem a classe $0$ e $n - k$ possuem a classe $1$. O algoritmo executa em três grandes etapas:

\begin{enumerate}
    \item particionar a base de treinamento em duas bases $S_0$ e $S_1$; $S_0$ possui as instâncias de classe $0$ e $S_1$ possui as instâncias de classe $1$;
    \item combinar as partições $S_0$ e $S_1$ de forma que a base de treinamento $S'$ possua todos os pares entre instâncias de classe $1$ e de classe $0$;
    \item aplicar um algoritmo de aprendizado $A$ à base $S'$ obtendo um classificador $c$ como saída.
\end{enumerate}

Em questão de custo computacional, o tempo de execução do algoritmo \emph{AUC-Train} é composto pela soma dos tempos de particionamento da base de treinamento, da combinação entre as partições e da aplicação do algoritmo de aprendizado.

Por se tratar de uma técnica baseada em metaclassificação, não é possível determinar um custo computacional para a 3ª etapa, pois algoritmos de aprendizado para classificação distintos executam em tempos distintos. Portanto, o estudo da complexidade da técnica \emph{AUC-Train} será baseado na computação extra que as etapas 1 e 2 acrescentam ao aprendizado.

Para o particionamento da base, basta iterar uma vez pela base original incluindo as instâncias de classe $0$ no conjunto $S_0$ e as de classe $1$ no conjunto $S_1$, como mostrado na \emph{função \ref{func:particionar}}. Essa etapa executa sempre em tempo $O(n)$.

\begin{function}[h!]
    \Entrada{Conjuntos de instâncias $S$}
    \Saida{Partições de instâncias $S_0, S_1$}
    
    $S_0, S_1 \gets \emptyset$\;
    
    \ParaTodo{$i \in S$}{
        \eSe{$y_i = 0$}{
            $S_0 \gets S_0 \cup \{ i \}$\;
        }{
            $S_1 \gets S_1 \cup \{ i \}$\;
        }
    }
    
    \Retorna{$S_0, S_1$}

    \caption{particionar($S$)}
    \label{func:particionar}
\end{function}

Após o particionamento da base, gera-se o conjunto $S'$ a partir das combinações entre as partições $S_0$ e $S_1$. A \emph{função \ref{func:combinar}} é a encarregada de formar novas instâncias a partir da mesclagem das instâncias nas partições $S_0$ e $S_1$. Essa função deve ser executada duas vezes a fim de formar todos os pares.

\begin{function}[h!]
    \Entrada{Conjuntos de instâncias $S_0$ e $S_1$}
    \Saida{Combinação de $S_0$ com $S_1$}

    $S_C \gets \emptyset$\;

    \ParaTodo{$i \in S_0$}{
        \ParaTodo{$j \in S_1$}{
            $S_C \gets S_C \cup \{ mesclar(i, j) \}$\;
            $S_C \gets S_C \cup \{ mesclar(j, i) \}$
        }
    }

    \Retorna{$S_C$}

    \caption{combinar($S_0, S_1$)}
    \label{func:combinar}
\end{function}

A mesclagem das instâncias consiste em gerar uma nova instância a partir de duas outras instâncias, de acordo com o algoritmo original do \emph{AUC-Train (\ref{alg:auc-train})}. A \emph{função \ref{func:mesclar}} desempenha esse papel. Essa função executa sempre em tempo $O(1)$.

\begin{function}[h!]
    \Entrada{Instâncias $\alpha$ e $\beta$}
    \Saida{Nova instância, mescla de $\alpha$ com $\beta$}

    \Retorna{$\langle (x_{\alpha}, x_{\beta}), 1(y_{\alpha} < y_{\beta}) \rangle$}

    \caption{mesclar($\alpha, \beta$)}
    \label{func:mesclar}
\end{function}

Os casos extremos da \emph{função \ref{func:combinar}} ocorrem quando $k = 0$ ou $k = n$. Nesses casos, todas as instâncias pertencem à mesma classe e o algoritmo não conseguiria formar pares para treinar um classificador; logo, esses casos não estarão incluídos em nossos estudos.

O melhor caso ocorre, com tempo computacional de $O(n)$, quando $k = 1$ ou $k = n - 1$. Nesses casos, todas as instâncias pertencem à mesma classe, exceto por uma, dessa forma um dos loops executa apenas uma vez, enquanto o outro executa $n$ vezes.

No caso médio, parte-se da hipótese que todas as entradas são uniformemente prováveis. A probabilidade de uma entrada para um dado $k$ é $\frac{1}{n - 1}$ e a quantidade de iterações geradas por essa entrada é $k \cdot (n - k)$. Partindo desses dados, a complexidade do caso médio pode ser calculada pela fórmula:

\[f_{avg}(n) = \sum_{k = 1}^{n - 1} \frac{k \cdot (n - k)}{n - 1}\]

Desenvolvendo o somatório:

\begin{align*}
    f_{avg}(n) &= \frac{1}{n - 1} \cdot \sum_{k = 1}^{n - 1} k(n - k) \\
               &= \frac{1}{n - 1} \cdot \sum_{k = 1}^{n - 1} kn - k^2 \\
               &= \frac{1}{n - 1} \cdot \left(\sum_{k = 1}^{n - 1} kn - \sum_{k = 1}^{n - 1} k^2\right) \\
               &= \frac{1}{n - 1} \cdot \left(n \cdot \sum_{k = 1}^{n - 1} k - \sum_{k = 1}^{n - 1} k^2\right) \\
               &= \frac{1}{n - 1} \cdot \left(n \cdot \frac{n((n - 1) + 1)}{2} - \frac{(n - 1)((n - 1) + 1)(2(n - 1) + 1)}{6}\right) \\
               &= \frac{1}{n - 1} \cdot \frac{n^3 - n}{6} \\
               &= \frac{n^2 + n}{6}
\end{align*}

Logo, a complexidade do caso médio é igual a $O(f_{avg}) = O(\frac{n^2 + n}{6}) = O(n^2)$.

O pior caso acontece quando $k = \frac{n}{2}$, nesse caso a base de treinamento possui metade das instâncias com classe $0$ e a outra metade com classe $1$. A combinação ocorrerá entre dois conjuntos com $\frac{n}{2}$ instâncias, resultando na complexidade assintótica $O(\frac{n}{2} \cdot \frac{n}{2}) = O(\frac{n^2}{4}) = O(n^2)$. Analisando a tabela \ref{auc-train-complexity}, percebe-se que o gargalo do algoritmo \emph{AUC-Train} reside na combinação das instâncias.

\begin{table}[h!]
    \centering
    \begin{tabular}{ c | c c | c }
        \hline

        Caso & Particionar & Combinar & AUC-Train \\

        \hline

        Melhor & $O(n)$ & $O(n)$ & $O(n) + O(n) = O(n)$ \\
        Médio & $O(n)$ & $O(n^2)$ & $O(n) + O(n^2) = O(n^2)$ \\
        Pior  & $O(n)$ & $O(n^2)$ & $O(n) + O(n^2) = O(n^2)$ \\

        \hline
    \end{tabular}

    \caption{Complexidade do algoritmo AUC-Train}
    \label{auc-train-complexity}
\end{table}


\section{Treinamento: Otimizações e implementação do algoritmo \emph{AUC-Train}}

Recapitulando, o algoritmo \ref{alg:auc-train} \emph{(AUC-Train)} é composto de três etapas principais: particionamento, com a \emph{função \ref{func:particionar}}; combinação das partições, com a \emph{função \ref{func:combinar}}; e treinamento do classificador.

De acordo com o estudo de complexidade , verificou-se que o custo computacional adicionado pelas duas primeiras etapas do algoritmo  original, principalmente a de combinação, é alto. Nós propomos duas estratégias, familiares no campo de aprendizado de máquina, para mitigar esse custo: Amostragem e Votação.

Amostragem consiste em utilizar apenas um subconjunto da base completa a ser usada no treinamento do classificador. O objetivo da estratégia não é reduzir a complexidade intríseca das etapas de particionamento e combinação, mas limitar a quantidade de instâncias a serem combinadas amortizando o que seria o tempo total de geração das novas instâncias. A \emph{função \ref{func:amostragem}} ilustra o processo de amostragem utilizado nesse estudo.

\begin{function}[h!]
    \Entrada{
        Partições de instâncias $S_{\alpha}$, $S_{\beta}$\;
        Pares por instância $p$
    }
    \Saida{Amostra com pares $S$}

    $S_a \gets \emptyset$\;
    
    \ParaTodo{$\alpha \in S_{\alpha}$}{
        \ParaTodo{$\beta \in SS_{\beta} \; tal\; que\; SS_{\beta} \subseteq S_{\beta} \; e \; |SS_{\beta}| = p$}{
            $S_a \gets S_a \cup \{ mesclar(\alpha, \beta) \}$\;
        }
    }

    \Retorna{$S_a$}

    \caption{amostragem($S_{\alpha}, S_{\beta}, p$)}
    \label{func:amostragem}
\end{function}

Votação consiste em treinar vários classificadores em vez de apenas um. Para avaliar uma instância, basta submetê-la aos classificadores e coletar as previsões. A classe que mais figurar entre as previsões é então a classe da instância.

Não há sentido em executar uma votação em que os classificadores foram aprendidos a partir da mesma base de treinamento. Isso resultaria em classificadores iguais e, portanto, seus votos também seriam iguais. Esse panorama só deixaria de ocorrer se o próprio algoritmo de aprendizado inserisse alguma incerteza durante o treinamento.

Apesar de o uso da amostragem beneficiar o tempo de execução, o classificador resultante não será treinado com todas as informações disponíveis, o que pode causar um efeito de intensificação dos erros de classificação. Com a votação, embora o treinamento de diversos classificadores resulte em acréscimo de tempo para o algoritmo, geralmente o resultado da classificação possuirá menos erros.

Há, assim, uma complementaridade entre as duas estratégias escolhidas para otimização do algoritmo \ref{alg:auc-train} \emph{(AUC-Train)}. Enquanto uma preza por melhorar o tempo de execução, o outra preza por dar maior segurança na classificação. Basta combiná-las para balancear o compromisso entre tempo de execução e redução dos erros de classificação.

De posse das informações necessárias para a implementação do algoritmo \ref{alg:auc-train} \emph{(AUC-Train)} e de suas otimizações, a etapa de treinamento do \emph{Ranking} está exposta no algoritmo \ref{alg:treinamento}.

\begin{algorithm}[h!]
    \Entrada{
        Base de treinamento $S$\;
        Algoritmo de aprendizado para classificação $A$\;
        Quantidade de classificadores a serem treinados $n$\;
        Pares por instância $p$
    }
    \Saida{Conjunto de classificadores $C$}

    $C \gets \emptyset$\;
    $S_0, S_1 \gets particionar(S)$;

    \Se{$n = 1$ e $p = todas$}{
        $S' \gets combinar(S_0, S_1)$\;
        $C \gets \{ A(S') \}$\;
    }
    \SenaoSe{$n > 1$ e $0 < p \leq min(|S_0|, |S_1|)$}{
        \Para{$i \gets 1; i \to n$}{
            $S' \gets amostragem(S_0, S_1, p)$\;
            $S' \gets S' \cup amostragem(S_1, S_0, p)$\;
            $C \gets C \cup \{ A(S') \}$\;
        }
    }

    \Retorna{$C$}

    \caption{Treinamento}
    \label{alg:treinamento}
\end{algorithm}

O parâmetro $p$ --- pares por instância --- representa a quantidade de pares que uma instância $i$ deve formar como primeiro item da combinação entre $i$ e $j$ onde $j$ é qualquer instância de classe antagônica à classe de $i$. O parâmetro $n$ indicarepresenta o número de classificadores a serem treinados para votação. O valor de retorno é sempre um conjunto de classificadores.

O algoritmo \ref{alg:treinamento} atende, através da manipulação dos parâmetros $n$ e $p$, tanto ao propósito do algoritmo \ref{alg:auc-train} \emph{(AUC-Train)} quanto às otimizações propostas para esse. Para executar o algoritmo original, basta definir $n$ como $1$ e $p$ como $todas$. Para executar o algoritmo com amostragens e votação, basta definir $n$ com um valor maior que $1$ e $p$ com um valor entre $1$ e o número de instâncias da classe minoritária.


\section{Ordenação: Estudo de complexidade do algoritmo \emph{Tournament}}

Após a etapa de treinamento, a técnica de \emph{ranking reduzido a classificação} procede à etapa de ordenação. Nessa etapa, o classificador aprendido na etapa de treinamento é utilizado para ordenar um conjunto de instâncias como descrito no algoritmo \ref{alg:tournament} \emph{(Tournament)}.

O algoritmo \ref{alg:tournament} \emph{(Tournament)} recebe como entrada um classificador $c$ aprendido durante a etapa de treinamento e uma base $U$ composta por $n$ instâncias sem classes assinaladas. Esse algoritmo executa em duas grandes etapas:

\begin{enumerate}
    \item Obter a pontuação para todas as instâncias na base $U$;
    \item Ordenar as instâncias na base $U$.
\end{enumerate}

Partindo dessa divisão, o tempo computacional do algoritmo é a soma dos tempos das etapas de obtenção das pontuações e de ordenação. A etapa de obtenção das pontuações é bem definida no algoritmo \ref{alg:tournament} \emph{(Tournament)}, enquanto, na etapa de ordenação, qualquer algoritmo de ordenação pode ser usado: \emph{mergesort} e \emph{quicksort}, para citar alguns.

A parte do algoritmo que calcula as pontuações envolve uma chamada à \emph{função \ref{func:votacao}}, que invoca o classificador base, com dois \emph{loops}. Portanto, o tempo de execução da etepa de obtenção da pontuação é composto pela quantidade de execuções dos loops multiplicada pelo tempo de execução da chamada à \emph{\ref{func:votacao}}.

Não é possível dizer exatamente qual é o limite superior de tempo de execução para o cálculo de todas as pontuações, pois classificadores distintos executam a classificação de um elemento em tempos distintos. Uma alternativa é, em vez de medir o tempo computacional, medir o limite superior de chamadas à \emph{função \ref{func:votacao}}, já que nela reside a incerteza da medida.

O pior caso, o caso médio e o melhor caso apresentam o mesmo limite superior de chamadas à \emph{função \ref{func:votacao}}, pois é sempre necessário promover o embate de todas as instâncias. Como a chamada à função está englobada por dois loops aninhados, isso resulta em um limite de chamadas de ordem $O(n^2)$. Dessa forma, a única afirmação que pode ser feita sobre o tempo de execução da etapa de pontuação é que ele pode ser maior ou igual a $O(n^2)$.

Como existem algoritmos de ordenação com tempo computacional da ordem de $O(n^2)$ para o pior caso, o gargalo da etapa de ordenação reside na necessidade de se gerar todos os pares a partir das instâncias no conjunto a ser ordenado. Qualquer tentativa de otimização desse processo deve levar em consideração essa observação.


\section{Ordenação: Otimizações e implementação do algoritmo \emph{Tournament}}

Vale destacar que a otimização de votação, sugerida para a etapa de treinamento, tem extensões na etapa de ordenação. Para que a estratégia de votação funcione, o algoritmo de ordenação precisa utilizar a \emph{função \ref{func:votacao}}. Essa iterará sobre o conjunto de classificadores produzidos na etapa de treinamento, determinando a classe majoritária na votação.

\begin{function}[h!]
    \Entrada{
        Conjunto de classificadores $C$\;
        Instância a ser classificada $i$\;
    }
    \Saida{Previsão da classe de $i$}

    $classe \gets 0$\;
    $zero \gets 0$\;

    \ParaTodo{$c \in C$}{
        \eSe{$c(i) = 0$}{
            $zero \gets zero + 1$\;
        }{
            $zero \gets zero - 1$\;
        }
    }

    \Se{$zero < 0$}{
        $classe \gets 1$\;
    }

    \Retorna{$classe$}

    \caption{votacao(C, i)}
    \label{func:votacao}
\end{function}

De acordo com o exposto até agora sobre a etapa de ordenação, essa pode ser descrita como no algoritmo \ref{alg:ordenacao}. Devido às especificações expostas, o algoritmo de ordenação é mais monolítico que o algoritmo de treinamento, que pode ser dividido em funções menores.

\begin{algorithm}[h!]
    \Entrada{
        Conjunto de classificadores $C$\;
        Base composta por instâncias sem classes assinaladas $U$\;
        Mapeamento das instâncias de $U$ com seus graus $grau$
    }
    \Saida{\emph{Ranking} da base $U$}

    \ParaTodo{$\alpha \in U$}{
        \ParaTodo{$\beta \in U \; e \; \beta \neq \alpha$}{
            $i \gets \langle (x_{\alpha}, x_{\beta}) \rangle$\;
            $classe \gets votacao(C, i)$\;

            \eSe{$classe = 1$}{
                $grau[\alpha] \gets grau[\alpha] + 1$\;
            }{
                $grau[\beta] \gets grau[\beta] + 1$\;
            }
        }
    }

    ordenar $U$ com base em $grau$\;

    \caption{Ordenação}
    \label{alg:ordenacao}
\end{algorithm}

Como visto na seção anterior, o custo computacional da etapa de ordenação é composto pelas somas dos custos da obtenção das pontuações e da ordenação. A maior parte do custo é devida ao processo de obtenção das pontuações, de complexidade maior ou igual a $O(n^2)$. E com as pontuações obtidas, ainda é executado um algoritmo de ordenação.

Uma otimização possível, segundo \cite{ailon08}, é utilizar o classificador treinado como uma relação de ordem a fim de estabelecer uma precedência entre as instâncias. Assim, seria possível utilizar o classificador diretamente no algoritmo de ordenação, em vez de coletar a pontuação de todas as instâncias.

Essa otimização eliminaria a necessidade da etapa de pontuação. O artigo sugere que o classificador seja usado, em conjunto com o algoritmo de \emph{quicksort}, como uma operação de comparação entre duas instâncias. A comparação ocorre no momento do particionamento do conjunto em que ocorre o reposicionamento do elemento pivot.

Combinado diretamente com o classificador, o algoritmo de ordenação quicksort ainda implica um limite superior de chamadas à \emph{função \ref{func:votacao}} de $O(n^2)$ no pior caso, porém reduz o limite para $O(n \cdot log(n))$ no caso médio e no melhor caso.

Ainda segundo \cite{ailon08}, uma consequência dessa otimização é tornar viável o uso da técnica de \emph{ranking reduzido a classificação} em sistemas comerciais, tais como: motores de busca e sistemas de recomendações.

Embora a ordenação por quicksort tenha sido implantada na solução para o \emph{workbench WEKA}, não houve nenhuma avaliação empírica dessa estratégia, como será visto no próximo capítulo.

% Tal implementação está explicada no algoritmo \ref{alg:quicksort}. O algoritmo assume que as instâncias na base podem ser acessadas através de sua posição.

% \begin{algorithm}
%     \Entrada{Base a ser ordenada $B$}
%     \Saida{Base ordenada $B$}
% 
%     $esquerda \gets 0$\;
%     $direita \gets |B| - 1$
% 
%     \Se{$esquerda < direita$}{
%         $indicePivot \gets escolherPivot(B)$\;
%         $novoIndicePivot \gets particionarQuicksort(B, esquerda, direita, indicePivot)$\;
%         $quicksort(B, esquerda, novoIndicePivot - 1)$\;
%         $quicksort(B, novoIndicePivot + 1, direita)$
%     }
% 
%     \caption{Quicksort}
%     \label{alg:quicksort}
% \end{algorithm}
% 
% \begin{function}
%     \Entrada{
%         Base a ser particionada $B$\;
%         Limites de particionamento $esquerda$ e $direita$\;
%         Indice do elemento pivot $indicePivot$
%     }
%     \Saida{Novo índice do elemento pivot $novoIndicePivot$}
% 
%     $instanciaPivot \gets B[indicePivot]$\;
%     $trocar(B[indicePivot], B[direita])$\;
%     $novoIndicePivot \gets esquerda$\;
%     
%     \Para{$i \gets esquerda; i \to direita - 1$}{
%         $instancia \gets B[i]$\;
%         $par \gets instanciaPivot || instancia$
%         \Se{$votacao(par) = 1$}{
%             $trocar(B[i], B[novoIndicePivot])$\;
%             $novoIndicePivot \gets novoIndicePivot + 1$
%         }
%     }
% 
%     $trocar(B[direita], B[novoIndicePivot])$\;
%     
%     \Retorna{$novoIndicePivot$}
% 
%     \caption{particionarQuicksort($B, esquerda, direita, indicePivot$)}
%     \label{func:particionar-quicksort}
% \end{function}