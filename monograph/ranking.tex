Como dito anteriormente, a técnica de \emph{ranking reduzido a classificação} aplica uma transformação à base original antes da etapa de treinamento do classificador e modifica a etapa de avaliação a fim de ordenar as instâncias. A transformação da base e o treinamento do classificador compõe um algoritmo de nome \emph{AUC-Train} (algoritmo \ref{alg:auc-train}).

\begin{algorithm}
    Let $S' = \{\langle (x_1, x_2), 1(y_1 < y_2) \rangle : (x_1, y_1), (x_2, y_2) \in S and y_1 \neq y_2\}$\;
    \Return{$c = A(S')$}
    
    \caption{AUC-Train}
    \label{alg:auc-train}
\end{algorithm}

A transformação consiste em formar novas instâncias a partir de pares entre instâncias de classes diferentes. A ordem em que as instâncias aparecem no par é relevante; o par $\langle i_{\alpha}, i_{\beta} \rangle$ e o par $\langle i_{\beta}, i_{\alpha} \rangle$ representam informações diferentes para o aprendizado do classificador. Além disso, as classes das duas instâncias que formam um par são combinadas a fim de gerar uma nova classe para a nova instância.

Essa transformação foi implantada particionando a base de treinamento em dois subconjuntos, um subconjunto $S_0$ com as instâncias de classe $0$ e outro, $S_1$, com as instâncias de classe $1$. Com as duas partições definidas, basta combiná-las em um conjunto de elementos com formato final $\langle (i_{\alpha}(A), i{\beta}(A)), 1(i_{\alpha}(C) < i_{\beta}(C)) \rangle$.

A \emph{função $\ref{func:one}$}, aplicada para obter a classe da nova instância, retorna $1$, se o resultado da expressão avaliada é verdadeiro, ou $0$, se o resultado da expressão é falsa. Como o argumento para a função é $i_{\alpha}(C) < i_{\beta}(C)$, se $i_{\alpha}(C)$ valer $1$, a classe da nova instância será $0$, se valer $0$, a classe da nova instância será $1$.

\begin{function}
    $avaliacao \gets 1$

    \Se{$expr = False$}{
        $avaliacao \gets 0$
    }

    \Retorna{$avaliacao$}

    \caption{1($expr$)}
    \label{func:one}
\end{function}

A modificação da etapa de avaliação tem como objetivo usar as previsões do classificador treinado pelo \emph{algoritmo \emph{AUC-Train}} para gerar o \emph{ranking} da base de avaliação. O algoritmo que efetua a ordenação recebe o nome de \emph{Tournament}, torneio em português. O porquê do nome é devido ao algoritmo se assemelhar muito a um torneio em que muitos competidores se enfrentam, formando um \emph{ranking} de acordo com suas performances.

Continuando a analogia com uma competição, a performance de cada instância na avaliação é medida como a quantidade de "vitórias" que essa instância obteve diante das outras instâncias na base de avaliação. Para gerar o \emph{ranking}, basta promover o embate entre todas as instâncias na base e ordenar pelo número de "vitórias".

Como desejamos ordenar as instâncias de forma que as que tenham maior chance de pertencer à classe $0$ estejam no topo, a semântica de "vitoria" é: em um embate entre duas instâncias, a instância vencedora é a que possui a maior chance de pertencer à classe $0$. Quem define a instância vitoriosa em um embate é a previsão do classificador binário.

Tecnicamente, um embate entre as instâncias $i_\alpha$ e $i_\beta$ consiste em criar uma nova instância com formato $\langle (i_{\alpha}(A), i_{\beta}(A))\rangle$ e submetê-la à classificação; se a classe prevista da nova instância for $1$, $i_{\alpha}$ ganha, se for $0$, $i_{\beta}$ ganha. Nota-se que o formato da instância a ser classificada é o mesmo que o de uma instância usada no treinamento do classificador, exceto pela ausência de classe. O algoritmo \ref{alg:tournament} demonstra o processo de ordenação.

\begin{algorithm}
    For $x \in U$, let $deg(x) = |\{x':c(x, x') = 1, x' \in U\}|$
    Sort U in descending order of deg(x), breaking ties arbitrarily
    
    \caption{Tournament}
    \label{alg:tournament}
\end{algorithm}

Uma das vantagens da técnica de \emph{ranking reduzido a classificação} é que o uso de um algoritmo de aprendizado para classificação é transparente. Essa característica possibilitou o uso dos algoritmos de classificação existentes na ferramenta \emph{WEKA}.

O principal ponto negativo é a performance da técnica no que diz respeito a tempo. Os algoritmos \emph{AUC-Train (\ref{alg:auc-train})} e \emph{Tournament (\ref{alg:tournament})} que constituem o cerne da técnica de \emph{ranking reduzido a classificação} possuem elevada complexidade assintótica.


\section{Estudo da Complexidade Assintótica}

Assim como a maioria das técnicas de aprendizagem supervisionada, o \emph{ranking reduzido a classificação} pode ser dividido em duas partes: a primeira é o treinamento de um ordenador e a segunda é a avaliação do ordenador treinado.

Os principais algoritmos nas etapas de treinamento e avaliação são, respectivamente, \emph{AUC-Train} e \emph{Tournament}. A complexidade da técnica de \emph{ranking reduzido a classificação} será estudada a partir das complexidades desses dois algoritmos separadamente.

Para esse estudo, considera-se como entrada para os algoritmos uma base $S$ com $n$ instâncias das quais $k$ possuem a classe $0$ e $n - k$ possuem a classe $1$.

O algoritmo \ref{alg:auc-train} executa em três etapas:

\begin{enumerate}
    \item particionar a base de treinamento em duas bases $S_0$ e $S_1$; $S_0$ possui as instâncias de classe $0$ e $S_1$ possui as instâncias de classe $1$;
    \item combinar duas vezes as partições $S_0$ e $S_1$ de forma que o conjunto de treinamento possua todos os pares entre instâncias de classe $1$ e de classe $0$
\end{enumerate}

Em questão de custo computacional, o tempo de execução do algoritmo \emph{AUC-Train} é composto pela soma dos tempos de particionamento da base de treinamento e da aplicação de duas combinações entre as partições.

Para o particionamento da base, basta iterar uma vez pela base original incluindo as instâncias de classe $0$ no conjunto $S_0$ e as de classe $1$ no conjunto $S_1$, como mostrado na \emph{função \ref{func:particionar}}. Essa etapa executa sempre em tempo $O(n)$.

\begin{function}
    \Entrada{Conjuntos de instâncias $S$}
    \Saida{Partições de instâncias $S_0, S_1$}

        $S_0, S_1 \gets \emptyset$

        \ParaTodo{$i \in S$}{
            \eSe{$i(C) = 0$}{
                $S_0 \gets S_0 \cup {i}$
            }{
                $S_1 \gets S_1 \cup {i}$
            }
        }

        \Retorna{$S_0, S_1$}

    \caption{particionar($S$)}
    \label{func:particionar}
\end{function}

Após o particionamento da base, gera-se o conjunto $S'$ a partir das combinações entre as partições $S_0$ e $S_1$. A \emph{função \ref{func:combinar}} é a encarregada de formar novas instâncias a partir da mesclagem das instâncias nas partições $S_0$ e $S_1$.

\begin{function}
    \Entrada{Conjuntos de instâncias $S_{\alpha}$ e $S_{\beta}$}
    \Saida{Combinacao de $S_{\alpha}$ com $S_{\beta}$}

    $S_{\times} \gets \emptyset$

    \Se{$S_{\alpha} \neq \emptyset \wedge S_{\beta} \neq \emptyset$}{
        \ParaTodo{$\alpha \in S_{\alpha}$}{
            \ParaTodo{$\beta \in S_{\beta}$}{
                $S_C \gets S_C \cup \{ f(\alpha, \beta) \}$
            }
        }
    }

    \Retorna{$S_C$}

    \caption{combinar($S_{\alpha}, S_{\beta}, f$)}
    \label{func:combinar}
\end{function}

A mesclagem das instâncias consiste em gerar uma nova instância a partir de duas outras instâncias de acordo com o algoritmo original do \emph{AUC-Train (\ref{alg:auc-train})}. A \emph{função \ref{func:mesclar}} desempenha esse papel.

\begin{function}
    \Entrada{Instâncias $\alpha$ e $\beta$}
    \Saida{Nova instância, mescla de $\alpha$ com $\beta$}

    \Retorna{$\langle (\alpha(A) \cup \beta(A)), 1 \cdot (\alpha(C), \beta(C)) \rangle $}

    \caption{mesclar($\alpha, \beta$)}
    \label{func:mesclar}
\end{function}



Para a \emph{função \ref{func:combinar}}, o melhor caso ocorre quando $k = 0$ ou $k = n$ com tempo computacional de $O(1)$. Nesses casos, todas as instâncias pertencem à mesma classe e $S_{\alpha} = \emptyset$ ou $S_{\beta} = \emptyset$, dessa forma os loops não são executados.

No caso médio, parte-se da hipótese que todas as entradas são uniformemente prováveis. A probabilidade de uma entrada para um dado $k$ é $\frac{1}{n}$ e a quantidade de iterações geradas por essa entrada é $k \cdot (n - k)$. Partindo desses dados, a complexidade do caso médio pode ser calculada pela fórmula $O(f_{avg}(n))$, na qual:

\[f_{avg}(n) = \sum_{k = 0}^{n} \frac{k \cdot (n - k)}{n}\]

Desenvolvendo o somatório:

\begin{align*}
    f_{avg}(n) &= \frac{1}{n} \cdot \sum_{k = 0}^{n} k(n - k) \\
               &= \frac{1}{n} \cdot \sum_{k = 0}^{n} kn - k^2 \\
               &= \frac{1}{n} \cdot \left(\sum_{k = 0}^{n} kn - \sum_{k = 0}^{n} k^2\right) \\
               &= \frac{1}{n} \cdot \left(n \cdot \sum_{k = 1}^{n} k - \sum_{k = 1}^{n} k^2\right) \\
               &= \frac{1}{n} \cdot \left(n \cdot \frac{n(n + 1)}{2} - \frac{n(n + 1)(2n + 1)}{6}\right) \\
               &= \frac{1}{n} \cdot \frac{n^3 + n}{6} = \frac{n^2 -1}{6}
\end{align*}

Logo, a complexidade do caso médio é igual a $O(f_{avg}) = O(\frac{n^2 -1}{6}) = O(n^2)$.

O pior caso acontece quando $k = \frac{n}{2}$, nesse caso a base de treinamento possui metade das instâncias com classe $0$ e a outra metade com classe $1$. A combinação ocorrerá entre dois conjuntos com $\frac{n}{2}$ instâncias, resultando na complexidade assintótica $O(\frac{n}{2} \cdot \frac{n}{2}) = O(\frac{n^2}{4}) = O(n^2)$.

\begin{table}[h]
    \centering
    \begin{tabular}{ c | c c | c }
        \hline

        Caso & Particionar & Combinar & AUC-Train \\

        \hline

        Melhor & $O(n)$ & $O(1)$ & $O(n) + O(1) = O(n)$ \\
        Médio & $O(n)$ & $O(n^2)$ & $O(n) + O(n^2) = O(n^2)$ \\
        Pior  & $O(n)$ & $O(n^2)$ & $O(n) + O(n^2) = O(n^2)$ \\

        \hline
    \end{tabular}

    \caption{Complexidade do algoritmo AUC-Train}
    \label{auc-train-complexity}
\end{table}

Analisando a tabela \ref{auc-train-complexity}, percebe-se que o gargalo do algoritmo \emph{AUC-Train} reside na combinação das instâncias. Na prática, algumas otimizações foram aplicadas ao algoritmo final escrito para o \emph{workbench WEKA}:
O algoritmo de produto cartesiano foi levemente alterado para gerar tanto o par $(x, y)$ quanto o par $(y, x)$; isso elimina a necessidade da aplicação de dois produtos cartesianos. Apesar da aplicação dessas otimizações, a complexidade do algoritmo se manteve inalterada.

Para o algoritmo de torneio, o pior caso, o caso médio e o melhor caso apresentam o mesmo tempo de execução, pois é sempre necessário formar todos os pares de instâncias possíveis a fim de obter a ordem de todas as instâncias. Como mostrado no algoritmo \ref{alg:tournament}, a operação de gerar todos os pares aninha dois \emph{loops}o que resulta em um custo de ordem $O(n^2)$, onde $n$ é o tamanho da base a ser ordenada.

Tanto o algoritmo \emph{AUC-Train} quanto o \emph{Tournament} apresentam tempos elevados de execução. A prática de comparar as instâncias em pares onera tanto os processos de treinamento e de ordenação e, usando bases extensas, o tempo de execução do algoritmo degrada consideravelmente. Essa característica levou à busca de alternativas para tornar ambos treinamento e ordenação mais rápidos.

Duas abordagens foram consideradas para amortizar a complexidade do algoritmo: uma para a fase de treinamento e outra para a fase de geração do \emph{ranking}. Para a fase de classificação, foi pensada uma estratégia de amostragem da base original e votação entre classificadores e para a geração do \emph{ranking}, foi pensada uma estratégia de torneio baseada em \emph{quicksort}.

\section{Otimização do treinamento: Amostragem e Votação}
O algoritmo original chama a etapa de treinamento de AUC-Train. Nessa etapa, são feitas combinações entre as instâncias de classe $0$ e de classe $1$, como mostrado no \emph{algoritmo \ref{alg:auc-train}}.

Esse algoritmo de mesclagem das instâncias é executado uma vez e seu resultado é usado como massa de dados para um algoritmo de aprendizado; no caso específico dessa implantação, um classificador.

O custo total do AUC-Train é composto pelos custos da etapa de mesclagem e do algoritmo de aprendizagem somados. Vale ressaltar que a etapa de mesclagem tem um tempo maior de execução; a otimização endereça esse problema especificamente.

Na estratégia adotada para reduzir o tempo de mesclagem figuram duas técnicas conhecidas em \emph{data mining}, a primeira é efetuar uma amostragem do conjunto original e a segunda é promover uma votação entre vários classificadores treinados com as amostragens como massa de dados.

\begin{function}
    \Entrada{
        Partições de instâncias $S_{\alpha}$, $S_{\beta}$\;
        Pares por instância $p$\;
        Função para manipulacão das instâncias $f$
    }
    \Saida{Amostra com pares $S$}

    $Set \gets \emptyset$\;
    
    \ParaTodo{$\alpha \in S_{\alpha}$}{
        \ParaTodo{$\beta \in Subset_{\beta} \; tal\; que\; Subset_{\beta} \subseteq S_{\beta} \wedge |Subset_{\beta}| = p$}{
            $Set \gets Set \cup \{ f(\alpha, \beta) \}$\;
        }
    }

    \Retorna{$S_a$}

    \caption{amostragem($S_{\alpha}, S_{\beta}, p, f$)}
    \label{func:amostragem}
\end{function}

\begin{algorithm}
    \Entrada{
        Partições de instâncias $S_0$, $S_1$\;
        Algoritmo de aprendizagem para classificação $A$\;
        Quantidade de amostragens $n$\;
        Pares por instância $p$
    }
    \Saida{Conjunto de classificadores $C$}

    $C \gets \emptyset$

    \eSe{$i > 1$}{
        \Para{$i=1 \to n$}{
            $S' \gets amostragem(S_0, S_1, p, mesclar)$\;
            $C \gets C \cup \{ A(S') \}$
        }
    }{
        $S' \gets combinar(S_0, S_1, mesclar)$\;
        $C \gets \{ A(S') \}$
    }

    \Retorna{$C$}

    \caption{Treinamento}
    \label{alg:treinamento}
\end{algorithm}

\begin{algorithm}
    \Entrada{
        Conjunto de classificadores $C$\;
        Base com instâncias de classe desconhecida $S$
    }
    \Saida{\emph{Ranking} $O$ da base $B$}

    Seja $pontos$, um mapeamento das instâncias de $B$ com suas respectivas pontuações

    \ParaTodo{$\alpha \in B$}{
        \ParaTodo{$\beta \in B \wedge \beta \neq \alpha$}{
            $i \gets \langle (\alpha(A) \cup \beta(A)) \rangle$

            $classe \gets 0$\;
            $zero \gets 0$

            \ParaTodo{$c \in C$}{
                \eSe{$c(i) = 0$}{
                    $zero \gets zero + 1$
                }{
                    $zero \gets zero - 1$
                }
            }
    
            \Se{$zero < 0$}{
                $classe \gets 1$
            }
    
            \Retorna{$classe$}

            \eSe{$classe = 1$}{
                $pontos[\alpha] \gets pontos[\alpha] + 1$
            }{
                $pontos[\beta] \gets pontos[\beta] - 1$
            }
        }
    }

    ordenar $B$ com base em $pontos$

    \caption{Avaliação}
    \label{alg:avaliacao}
\end{algorithm}

O parâmetro de pares por instância indica a quantidade pares que uma instância $i$ deve formar como primeiro item da tupla $(i, y)$ onde $y$ é qualquer instância de classe antagônica à classe de $i$.

Supondo que todos os classificadores possuem a mesma configuração, não há sentido em executar uma votação em que os classificadores foram aprendidos a partir da base de treinamento completa. Isso resultaria em votos iguais para todos os classificadores.

O produto gerado pelo algoritmo \ref{alg:treinamento} é um conjunto de classificadores treinados a partir de amostragens do conjunto original. Se passarmos os parâmetros $limit=all$ e $iterations=1$, ele gera o mesmo resultado do algortimo descrito por {{langford}}.

\section{Otimização da ordenação: Quicksort}
A etapa de ordenação original é chamada de \emph{torneio}. Nessa abordagem, todas as instâncias do conjunto a ser ordenado são comparadas entre si com base no classificador obtido no treinamento e recebem uma pontuação. As instâncias de maior pontuação assumem as primeiras posições no \emph{ranking}. A proposta de otimização baseia-se em uma adaptação do algoritmo de quicksort para ordenar as instâncias do \emph{ranking}.